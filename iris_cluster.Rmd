---
title: "Clustering"
output: github_document
date: "2023-04-03"
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Iris Clustering

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```

## Loading the data set
```{r}
data <- iris %>% clean_names()
head(data)
```
Let's see which types of species of irises there are.
```{r}
unique(data$species)
```
We are going to do a binary classification, so let's only keep two of the species and set 
'setosa' rows to 1 and the other to 0
```{r}
dat <- data %>% 
  filter(species != "virginica") %>% 
  mutate(species_int = ifelse(species == "setosa", 0,1)) %>% 
  select(sepal_length, sepal_width, species, species_int)
```

## Supervised vs Unsupervised Learning

Let's compare using a method we already know to our new clustering method to determine which species the irises are. 

Before we do that. Let's create a test and training set.
```{r}
inds <- sample(1:nrow(dat),size = round(nrow(dat)*.7),replace = F)
train <- dat %>% slice(inds)
test <- dat %>% slice(-inds)
```

Now we will use a regression to determine the species!

### Supervised Learning - Classification (Logistic Regression)

Here, we are only going to use sepal length and width to determine species.
```{r warning=FALSE}
mLG <- glm(factor(species) ~ sepal_length + sepal_width, #formula. Determining species with                                                            sepal length and width  
           family = binomial(link = 'logit'), #Using a logit
           data = train) #training the model with the 'train' dataset

reg_test <- test %>% 
  mutate(proba = predict(mLG, newdata = test, type = "response"), #Predicting the values of                                                                    the 'test' dataset with our                                                                   trained model
         pred = ifelse(proba > 0.5,1,0), #Making a binary prediction
         truth = factor(species_int,levels = c('1','0'))) %>%
  select(species, sepal_length, sepal_width, prob=proba, pred, truth) #select and reorder                                                                          columns for clarity 

reg_test
```
```{r}
table(reg_test$truth, reg_test$pred) #Confusion matrix!!!
```

```{r}
roc_auc(reg_test,truth = 'truth',estimate = 'pred')
```
As we can see in the chunk above, the model fits very well. 

```{r}
ggplot(reg_test) +
  geom_point(aes(x = sepal_length, y = sepal_width, color = truth)) +
  labs(title = "Sepal Width vs Sepal Length", x = "Sepal Length", y = "Sepal Width")
```
Graph of our classification from regression.

Now, let's try our new clustering method!

### Unsupervised Learning - Clustering (K-Means Clustering)

So lets try and compare if our kmeans clustering method would classify the points differently than regression. 

To do this we are simply going to create clusters of our 'test' dataset with the kmeans algorithm, and see if those clusters are the same as how the regression classified the points. 

```{r}
clust_test <- test %>% 
  select(-species, truth=species_int) %>% #UNSUPERVISED LEARNING!!!
  mutate(method = "Clustering")
clust_test
```

  - `x` is the data (only select the columns of interest!)
  
  - `centers` is the number of centroids
  
  - `iter.max` maximum amount of "steps"
  
  - `nstart` how many times to re-estimate
  
### With 2 clusters


*Remember:* in supervised learning (regression), we train the model by knowing the actual value for each row and finding how the features determine that. With unsupervised learning (clustering), we are simply learning about the data without knowing the actual values. This is why we remove the 'truth' column in the chunk below. 
```{r}
set.seed(123)
m2 <- kmeans(clust_test%>%select(-method, -truth), centers = 2, nstart = 10) #train.
m2
```
Let's visualize our clusters!
```{r}
clust_test$class = factor(m2$cluster-1)
clust_test
```
```{r}
clust_test %>%
  ggplot(aes(x = sepal_length, y = sepal_width, color=class)) +
  geom_point() +
  geom_point(data=data.frame(m2$centers),
             aes(x=sepal_length, y=sepal_width),
             inherit.aes = F, shape = '+',size = 10)
  labs(title = "Sepal Width vs Sepal Length")
```
Evaluate the fit with the withinss value.
```{r}
sum(tidy(m2)$withinss)
```
Now, let's compare these classifications to the regression predictions. 
```{r}
facet_reg_test = reg_test %>%
  select(sepal_length, sepal_width, class=pred) %>%
  mutate(method="Regression", class=factor(class))

comparison = bind_rows(facet_reg_test, clust_test)

comparison %>%
  ggplot(aes(x=sepal_length, y=sepal_width, color=class)) +
  geom_point() +
  facet_wrap(~method)
```
As we can see, these are not exactly the same result. The clustering method classified the several of the points in the middle and bottom left differently from the regression method.  

Let's see the confusion matrix for the clusters.
```{r}
table(clust_test$truth, clust_test$class) #Confusion matrix!!!
```
Looking at the CM for the regression model:
```{r}
table(reg_test$truth, reg_test$pred) #Confusion matrix!!!
```
*IN CONCLUSION:* We can see that clustering did slightly worse. That being said, let's remember clustering never saw the labels for any of the data. It simply learned from the test set and algorithmically put the points in groups. This is un

Now that we can see the clusters, let's predict our test set and see if kmeans, classifies the points the same way the regression did. 


HOPEFULLY THAT WAS HELPFUL IN BETTER EXPLAINING CLUSTERING WHICH IS AN UNSUPERVISED LEARNING METHOD.


Now, let's remember that initially, our first dataset had three types of flowers. So let's reuse that dataset and see how kmeans classify those. 

### With 3 clusters
```{r}
data
```

```{r}
datClust = data %>%
  select(species, sepal_length, sepal_width) %>%
  mutate(truth = ifelse(species=="setosa", 1,
                        ifelse(species=="versicolor", 2, 3)))
m3 <- kmeans(datClust%>%select(-species, -truth), centers = 3,nstart = 10)
m3
```

```{r}
datClust <- datClust %>% 
  mutate(m3cluster = factor(m3$cluster))

cents3 <- data.frame(m3$centers)

ggplot(datClust) +
  geom_point(aes(x = sepal_length, y = sepal_width, color = m3cluster)) +
  geom_point(data = cents3, aes(x = sepal_length,y = sepal_width),
             inherit.aes = F,shape = '+',size = 10) +
  labs(title = "Sepal Width vs Sepal Length")
```
```{r}
tidy(m3)
```

```{r}
sum(tidy(m3)$withinss)
```
Let's see how it did. 
```{r}
table(datClust$truth, datClust$m3cluster)
```
It appears the kmeans did a pretty good job!

```{r}
library(plotly)
cents <- data.frame(m3$centers)

ggClust <- ggplot(datClust,
                  aes(x = sepal_length, y = sepal_width,
                      color = m3cluster, text = species)) +
  geom_point() +
  labs(title = "Sepal Width vs Sepal Length") +
  scale_color_discrete(labels=c("setosa", "versicolor", "virginica")) 
  

ggplotly(ggClust,tooltip = 'text')
```

Finally, lets see how kmeans, would classify these flowers into 4 clusters, even though we know there are only 3 clusters. 
### With 4 clusters

```{r}
datClust = data %>%
  select(species, sepal_length, sepal_width) %>%
  mutate(truth = ifelse(species=="setosa", 1,
                        ifelse(species=="versicolor", 2, 3)))

m4 <- kmeans(datClust%>%select(-species, -truth), centers = 4,nstart = 10)
m4
```

```{r}
datClust <- datClust %>% 
  mutate(m4cluster = factor(m4$cluster))

cents4 <- data.frame(m4$centers)

ggplot(datClust) +
  geom_point(aes(x = sepal_length, y = sepal_width, color = m4cluster)) +
  geom_point(data = cents4, aes(x = sepal_length,y = sepal_width),
             inherit.aes = F,shape = '+',size = 10) +
  labs(title = "Sepal Width vs Sepal Length")
```

```{r}
tidy(m4)
```
```{r}
sum(tidy(m4)$withinss)
```


```{r}
datClust = data %>%
  select(species, sepal_length, sepal_width) %>%
  mutate(truth = ifelse(species=="setosa", 1,
                        ifelse(species=="versicolor", 2, 3)))
totWSS <- NULL
for(k in 1:10) {
  m.cluster <- datClust %>% select(contains("sepal")) %>% kmeans(centers = k,nstart = 10)
  totWSS <- data.frame(totWSS = m.cluster$tot.withinss,k = k) %>%
    bind_rows(totWSS)
}
```

#### Choosing the right "ELBOW"

```{r}
totWSS %>%
  ggplot(aes(x = k,y = totWSS)) + 
  geom_line() + geom_point() + 
  labs(x = 'Number of Clusters',y = 'Total WSS') + 
  scale_x_continuous(breaks = 1:10)
```

### Now let's practice using our method on a new batch of data

### New Data

```{r}
hw <- read_csv("https://raw.githubusercontent.com/MUbarak123-56/DataBEL/master/SOCR-HeightWeight.csv") %>%  clean_names()
```

```{r}
glimpse(hw)
```

```{r}
head(hw)
```

```{r}
inds <- sample(1:nrow(hw),size = round(nrow(hw)*.1),replace = F)
hw_new <- hw %>% slice(inds)
```

```{r}
ggplot(hw_new) +
  geom_point(aes(x = height_inches, y= weight_pounds)) +
  labs(title = "Weight vs Height", x = "Height", y = "Weight")
```

```{r}
totWSS <- NULL
for(k in 1:10) {
  m.cluster <- hw_new %>% select(height_inches, weight_pounds) %>%  kmeans(centers = k,nstart = 10)
  totWSS <- data.frame(totWSS = m.cluster$tot.withinss,k = k) %>%
    bind_rows(totWSS)
}
```

```{r}
totWSS %>%
  ggplot(aes(x = k,y = totWSS)) + 
  geom_line() + geom_point() + 
  labs(x = 'Number of Clusters',y = 'Total WSS') + 
  scale_x_continuous(breaks = 1:10)
```

```{r}
m2 <- kmeans(hw_new %>%  select(height_inches, weight_pounds), centers = 2)
tidy(m2)
```

```{r}
hw_new <- hw_new %>% 
  mutate(cluster = factor(m2$cluster))

cents <- data.frame(m2$centers)

ggplot(hw_new) + 
  geom_point(aes(x = height_inches, y = weight_pounds, color = cluster)) +
  geom_point(data = cents, aes(x = height_inches,y = weight_pounds),
             inherit.aes = F,shape = '+',size = 10) +
  labs(title = "Weight vs Height", x = "Height", y = "Weight") 
```

```{r}
m3 <- kmeans(hw_new %>%  select(height_inches, weight_pounds), centers = 3)
tidy(m3)
```

```{r}
hw_new <- hw_new %>% 
  mutate(cluster3 = factor(m3$cluster))

cents <- data.frame(m3$centers)

ggplot(hw_new) + 
  geom_point(aes(x = height_inches, y = weight_pounds, color = cluster3)) +
  geom_point(data = cents, aes(x = height_inches,y = weight_pounds),
             inherit.aes = F,shape = '+',size = 10) +
  labs(title = "Weight vs Height", x = "Height", y = "Weight")
```

